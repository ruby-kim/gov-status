import csv
import json
import aiohttp
import asyncio
import time
from urllib.parse import urlparse
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Any
import os
import redis
from collections import defaultdict
import uuid

TIMEOUT_THRESHOLD= 30000

class GovSiteStatusCheckerAsync:
    def __init__(self, csv_file: str = 'tasks/gov_sites.csv'):
        self.csv_file = csv_file
        self.results: List[Dict[str, Any]] = []

        # ê¸°ê´€ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ìš© í‚¤ì›Œë“œ
        self.central_agencies = ['ë¶€', 'ì²­', 'ìœ„ì›íšŒ', 'ì²˜', 'ì›', 'ê°ì‚¬ì›']
        self.local_agencies = ['ì‹œ', 'ë„', 'êµ¬', 'êµ°', 'íŠ¹ë³„ì‹œ', 'ê´‘ì—­ì‹œ', 'íŠ¹ë³„ìì¹˜ì‹œ', 'íŠ¹ë³„ìì¹˜ë„']

        # Redis ì—°ê²°
        redis_url = os.getenv("REDIS_URL")
        if not redis_url:
            raise ValueError("âŒ REDIS_URL í™˜ê²½ ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        self.redis = redis.from_url(redis_url)

        # ì ê²€ í˜ì´ì§€ í‚¤ì›Œë“œ
        self.maintenance_keywords = ["ì ê²€", "ì¼ì‹œì¤‘ë‹¨", "ì„œë¹„ìŠ¤ì¤‘ë‹¨", "maintenance"]

    def classify_agency(self, agency_name: str) -> Dict[str, str]:
        agency_name = agency_name.strip()
        for keyword in self.central_agencies:
            if keyword in agency_name:
                return {"mainCategory": "ì¤‘ì•™í–‰ì •ê¸°ê´€", "subCategory": self.get_sub_category(agency_name)}
        for keyword in self.local_agencies:
            if keyword in agency_name:
                return {"mainCategory": "ì§€ë°©ìì¹˜ë‹¨ì²´", "subCategory": self.get_local_sub_category(agency_name)}
        return {"mainCategory": "ì¤‘ì•™í–‰ì •ê¸°ê´€", "subCategory": "ê¸°íƒ€"}

    def get_sub_category(self, agency_name: str) -> str:
        if 'ë¶€' in agency_name and 'ìœ„ì›íšŒ' not in agency_name: return 'ë¶€'
        elif 'ì²­' in agency_name: return 'ì²­'
        elif 'ìœ„ì›íšŒ' in agency_name: return 'ìœ„ì›íšŒ'
        elif 'ì²˜' in agency_name: return 'ì²˜'
        else: return 'ê¸°íƒ€'

    def get_local_sub_category(self, agency_name: str) -> str:
        if 'íŠ¹ë³„ì‹œ' in agency_name or 'ê´‘ì—­ì‹œ' in agency_name: return 'ê´‘ì—­ì‹œ'
        elif 'íŠ¹ë³„ìì¹˜ì‹œ' in agency_name: return 'íŠ¹ë³„ìì¹˜ì‹œ'
        elif 'ë„' in agency_name and 'íŠ¹ë³„ìì¹˜ë„' not in agency_name: return 'ë„'
        elif 'íŠ¹ë³„ìì¹˜ë„' in agency_name: return 'íŠ¹ë³„ìì¹˜ë„'
        elif 'êµ¬' in agency_name: return 'êµ¬'
        elif 'êµ°' in agency_name: return 'êµ°'
        else: return 'ì‹œ'

    def generate_tags(self, agency_name: str, url: str) -> List[str]:
        tags = []
        if 'ë¶€' in agency_name: tags.append('ë¶€')
        elif 'ì²­' in agency_name: tags.append('ì²­')
        elif 'ìœ„ì›íšŒ' in agency_name: tags.append('ìœ„ì›íšŒ')
        elif 'ì‹œ' in agency_name or 'ë„' in agency_name: tags.append('ì§€ë°©ìì¹˜ë‹¨ì²´')

        domain = urlparse(url).netloc
        if '.go.kr' in domain: tags.append('ì •ë¶€ë„ë©”ì¸')
        elif '.ac.kr' in domain: tags.append('êµìœ¡ê¸°ê´€')
        elif '.re.kr' in domain: tags.append('ì—°êµ¬ê¸°ê´€')
        return tags if tags else ['ê³µê³µê¸°ê´€']

    async def check_site_status(self, session: aiohttp.ClientSession, semaphore: asyncio.Semaphore, agency_name: str, url: str) -> Dict[str, Any]:
        async with semaphore:
            start_time = time.monotonic()
            try:
                async with session.get(url, allow_redirects=True, timeout=aiohttp.ClientTimeout(total=10)) as response:
                    raw = await response.read()
                    text = raw.decode(errors="ignore")
                    response_time = int((time.monotonic() - start_time) * 1000)

                    if response.status == 200:
                        status = 'normal' if response_time < TIMEOUT_THRESHOLD else 'problem'
                    elif response.status == 503:
                        status = 'maintenance'
                    else:
                        status = 'problem'

                    if status == 'normal':
                        for kw in self.maintenance_keywords:
                            if kw.lower() in text.lower():
                                status = 'maintenance'
                                break
            except Exception as e:
                print(f"âŒ {agency_name} ìš”ì²­ ì‹¤íŒ¨: {e}")
                status = 'problem'
                response_time = TIMEOUT_THRESHOLD

            # URL ê¸°ë°˜ìœ¼ë¡œ ê³ ì • ID ìƒì„± (ì´ë¦„ì´ ë°”ë€Œì–´ë„ ìœ ì§€ë¨)
            agency_id = str(uuid.uuid5(uuid.NAMESPACE_URL, url))

            result = {
                'id': str(uuid.uuid4())[:8],  # ìš”ì²­ ì‹¤í–‰ ê³ ìœ  ID (ë¡œê·¸ìš©)
                'name': agency_name,
                'url': url,
                'status': status,
                'description': f'{agency_name} ê³µì‹ í™ˆí˜ì´ì§€',
                'agency': {
                    "id": agency_id,
                    "name": agency_name,
                    "url": url,
                    **self.classify_agency(agency_name)
                },
                'lastChecked': datetime.now(timezone.utc).isoformat(),
                'responseTime': response_time,
                'tags': self.generate_tags(agency_name, url)
            }

            print(f"{agency_name} [{url}] â†’ {status} ({response_time}ms)")
            return result

    def load_csv_data(self) -> List[List[str]]:
        data = []
        with open(self.csv_file, 'r', encoding='utf-8-sig') as f:
            reader = csv.reader(f)
            next(reader)
            for row in reader:
                if len(row) >= 2 and row[1].strip():
                    data.append(row)
        return data

    async def check_all_sites(self, concurrency: int = 30, batch_size: int = 100):
        data = self.load_csv_data()
        semaphore = asyncio.Semaphore(concurrency)

        async with aiohttp.ClientSession(headers={'User-Agent': 'GovStatusBot/1.0'}) as session:
            tasks = [self.check_site_status(session, semaphore, row[0], row[1]) for row in data]

            batch = []
            for coro in asyncio.as_completed(tasks):
                try:
                    result = await coro
                    if result:
                        self.results.append(result)
                        batch.append(result)

                    # ë°°ì¹˜ ë‹¨ìœ„ Redis ì €ì¥
                    if len(batch) >= batch_size:
                        self.save_partial_to_redis(batch)
                        batch = []
                except Exception as e:
                    print(f"âŒ ì‚¬ì´íŠ¸ í™•ì¸ ì‹¤íŒ¨: {e}")

            # ë‚¨ì€ ë°°ì¹˜ ì €ì¥
            if batch:
                self.save_partial_to_redis(batch)

    def save_partial_to_redis(self, batch_results: List[Dict[str, Any]]):
        """ë°°ì¹˜ ë‹¨ìœ„ë¡œ Redisì— ëˆ„ì  ì €ì¥"""
        pipe = self.redis.pipeline()
        for r in batch_results:
            pipe.rpush("services:stream", json.dumps(r, ensure_ascii=False))  # ìŠ¤íŠ¸ë¦¼ì²˜ëŸ¼ ì €ì¥
        pipe.execute()
        print(f"ğŸ“¥ Redisì— {len(batch_results)}ê°œ ì €ì¥ ì™„ë£Œ")

    def build_stats(self):
        overall = {"total": 0, "normal": 0, "maintenance": 0, "problem": 0}
        per_agency = {}

        for s in self.results:
            aid = s["agency"]["id"]
            if aid not in per_agency:
                per_agency[aid] = {
                    "id": aid,
                    "name": s["agency"]["name"],
                    "url": s["agency"]["url"],
                    "stats": {"total": 0, "normal": 0, "maintenance": 0, "problem": 0}
                }

            st = s["status"]
            per_agency[aid]["stats"]["total"] += 1
            per_agency[aid]["stats"][st] += 1
            overall["total"] += 1
            overall[st] += 1

        return {"timestamp": datetime.now(timezone.utc).isoformat(), "overall": overall, "perAgency": per_agency}

    def save_summary_to_redis(self):
        """ìµœì¢… í†µê³„ë§Œ ì €ì¥"""
        stats = self.build_stats()
        pipe = self.redis.pipeline()
        pipe.set("services:latest", json.dumps(self.results, ensure_ascii=False))
        pipe.set("stats:latest", json.dumps(stats, ensure_ascii=False))
        pipe.zadd("stats:history", {json.dumps(stats, ensure_ascii=False): datetime.now(timezone.utc).timestamp()})
        cutoff = datetime.now(timezone.utc) - timedelta(days=90)
        pipe.zremrangebyscore("stats:history", 0, cutoff.timestamp())
        pipe.execute()
        print("âœ… Redis ìµœì¢… ìš”ì•½ ì—…ë°ì´íŠ¸ ì™„ë£Œ")


async def main():
    checker = GovSiteStatusCheckerAsync()
    await checker.check_all_sites(concurrency=30, batch_size=100)
    checker.save_summary_to_redis()


if __name__ == "__main__":
    asyncio.run(main())
